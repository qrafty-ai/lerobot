---
phase: 02-learner-pi-rl-training-path
plan: 02
type: execute
wave: 2
depends_on:
  - 01
files_modified:
  - src/lerobot/rl/learner.py
  - src/lerobot/policies/xvla/modeling_xvla.py
  - src/lerobot/policies/factory.py
  - src/lerobot/configs/train.py
  - tests/rl/test_actor_learner.py
  - tests/rl/test_learner_service.py
autonomous: true
requirements:
  - LRN-01
  - LRN-02
must_haves:
  truths:
    - "PI-RL learner branch executes XVLA flow-matching update calls rather than SAC actor/critic updates."
    - "PI-RL update consumes current replay transition fields without transport schema changes."
    - "Default SAC code path remains operational when recipe is not `pi-rl`."
  artifacts:
    - path: src/lerobot/policies/xvla/modeling_xvla.py
      provides: PI-RL-compatible policy forward/update contract for XVLA
    - path: src/lerobot/rl/learner.py
      provides: integration of XVLA PI-RL forward/update calls into PI-RL branch
    - path: tests/rl/test_actor_learner.py
      provides: runtime branch behavior checks for XVLA + PI-RL path
  key_links:
    - from: src/lerobot/rl/learner.py
      to: src/lerobot/policies/xvla/modeling_xvla.py
      via: policy forward/update invocation from PI-RL learner branch
      pattern: "policy.forward|model=\"pi_rl\""
    - from: src/lerobot/rl/learner.py
      to: replay transition batch
      via: batch fields mapped into PI-RL forward contract
      pattern: "state|action|reward|next_state|done"
---

<objective>
Integrate PI-RL loss and optimizer flow for XVLA through an explicit policy-forward contract.

Purpose: Make the learner PI-RL branch perform real XVLA flow-matching RL optimization while preserving existing replay schema and SAC fallback.
Output: XVLA PI-RL forward/update contract wired into learner PI-RL branch with targeted runtime regression coverage.
</objective>

<execution_context>
@/home/cc/.config/opencode/get-shit-done/workflows/execute-plan.md
@/home/cc/.config/opencode/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-learner-pi-rl-training-path/02-CONTEXT.md
@.planning/phases/02-learner-pi-rl-training-path/02-RESEARCH.md
@.planning/phases/02-learner-pi-rl-training-path/02-01-SUMMARY.md
@src/lerobot/rl/learner.py
@src/lerobot/policies/xvla/modeling_xvla.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add XVLA PI-RL forward/update contract for recipe-specific optimization</name>
  <files>src/lerobot/policies/xvla/modeling_xvla.py, src/lerobot/policies/factory.py, tests/rl/test_learner_service.py</files>
  <action>Extend XVLA policy training interface with a PI-RL-specific forward/update mode (for example `model="pi_rl"`) that accepts batch transitions and returns explicit PI-RL loss outputs and metrics. Keep contract compatible with flow-noise and flow-sde recipe variants from existing PI-RL config, and avoid changing policy taxonomy or non-XVLA policy registration behavior.</action>
  <verify>Run `pytest -sv tests/rl/test_learner_service.py -k "pi_rl and xvla and forward"`.</verify>
  <done>XVLA exposes a stable PI-RL training contract callable from learner PI-RL branch without introducing `pi_rl` as a policy type.</done>
</task>

<task type="auto">
  <name>Task 2: Wire learner PI-RL branch to call XVLA PI-RL loss and optimizer step</name>
  <files>src/lerobot/rl/learner.py, src/lerobot/configs/train.py, tests/rl/test_actor_learner.py</files>
  <action>Replace PI-RL branch skeleton hooks from plan 02-01 with actual XVLA PI-RL forward call + optimizer step, including gradient clipping and metric logging aligned with existing learner conventions. Make optimizer construction explicit in learner code: when `recipe=pi-rl`, create a PI-RL optimizer set (for example under `optimizers["pi_rl"]`) sourced from explicit PI-RL hyperparameters in config; when recipe is not PI-RL, keep existing SAC optimizer construction unchanged. Ensure PI-RL branch consumes existing replay fields (`state`, `action`, `reward`, `next_state`, `done`, `complementary_info`) and does not alter transition transport format. Add targeted regression coverage proving PI-RL branch executes when recipe is `pi-rl` and SAC branch remains default otherwise.</action>
  <verify>Run `pytest -sv tests/rl/test_actor_learner.py -k "pi_rl or xvla or learner"` and confirm no SAC-path regressions in existing RL tests.</verify>
  <done>PI-RL recipe mode performs real XVLA PI-RL optimization in learner using existing replay batches; non-PI-RL runs keep SAC behavior.</done>
</task>

</tasks>

<verification>
- PI-RL learner branch invokes XVLA PI-RL forward contract with replay batch fields.
- Replay schema and transport paths remain unchanged.
- SAC default path still passes existing RL checks.
</verification>

<success_criteria>
- LRN-01 optimization branch is functionally integrated with XVLA.
- LRN-02 queue/replay compatibility remains intact.
- Recipe-layer behavior is preserved without policy taxonomy drift.
</success_criteria>

<output>
After completion, create `.planning/phases/02-learner-pi-rl-training-path/02-02-SUMMARY.md`
</output>
