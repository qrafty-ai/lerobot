---
phase: 02-learner-pi-rl-training-path
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/lerobot/rl/learner.py
  - src/lerobot/configs/train.py
  - tests/rl/test_learner_service.py
autonomous: true
requirements:
  - LRN-01
  - LRN-02
must_haves:
  truths:
    - "Learner enters a PI-RL optimization path when recipe is `pi-rl`."
    - "Non-PI-RL runs keep existing SAC optimization behavior unchanged."
    - "PI-RL path consumes transitions from existing replay/queue flow."
  artifacts:
    - path: src/lerobot/rl/learner.py
      provides: recipe-conditional optimization branch and update scheduling hooks
    - path: src/lerobot/configs/train.py
      provides: PI-RL runtime branch configuration defaults/validation helpers consumed by learner
    - path: tests/rl/test_learner_service.py
      provides: learner-side PI-RL branch routing coverage without hardware dependency
  key_links:
    - from: src/lerobot/rl/learner.py
      to: src/lerobot/configs/train.py
      via: recipe preflight context and config values
      pattern: "validate_recipe_runtime_preflight|build_recipe_preflight_context"
    - from: src/lerobot/rl/learner.py
      to: replay buffer iterators
      via: existing transition ingestion and sampling path
      pattern: "process_transitions|get_iterator"
---

<objective>
Implement learner-side PI-RL branch selection and scheduling hooks on top of the existing training loop.

Purpose: Satisfy recipe-driven learner activation without changing transport contracts or regressing SAC behavior.
Output: A recipe-gated PI-RL branch in the learner loop that uses current replay ingestion and explicit scheduling points for later PI-RL loss integration.
</objective>

<execution_context>
@/home/cc/.config/opencode/get-shit-done/workflows/execute-plan.md
@/home/cc/.config/opencode/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-learner-pi-rl-training-path/02-CONTEXT.md
@.planning/phases/02-learner-pi-rl-training-path/02-RESEARCH.md
@src/lerobot/rl/learner.py
@src/lerobot/configs/train.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add recipe-gated learner optimization branch in main training loop</name>
  <files>src/lerobot/rl/learner.py</files>
  <action>In `add_actor_information_and_train`, add explicit branching keyed by preflight recipe context (`pi-rl` vs default). Keep the existing SAC path as the unchanged default branch. Add a PI-RL branch skeleton that runs on the same sampled batches and preserves current loop lifecycle (transition ingest, interaction processing, save/log cadence, actor push cadence), so transport/replay behavior remains identical while update math can differ by recipe.</action>
  <verify>Run `pytest -sv tests/rl/test_learner_service.py -k "recipe or preflight or learner"` and verify non-PI-RL startup tests still pass.</verify>
  <done>Recipe `pi-rl` selects a distinct learner update path, and non-PI-RL continues using the existing SAC update path without behavior drift.</done>
</task>

<task type="auto">
  <name>Task 2: Introduce explicit PI-RL scheduling hooks tied to existing loop counters</name>
  <files>src/lerobot/rl/learner.py, src/lerobot/configs/train.py</files>
  <action>Add PI-RL scheduling hook points that use existing optimization/interaction counters and current `save_freq`/`policy_update_freq` conventions. Keep defaults smoke-friendly and deterministic, and avoid introducing new transport messages. Ensure schedule values are validated and only applied when recipe is `pi-rl` so default SAC config semantics remain untouched.</action>
  <verify>Run `pytest -sv tests/configs/test_train_config.py -k "recipe or pirl"` and a targeted learner unit invocation to confirm PI-RL schedule parsing works while SAC defaults remain unchanged.</verify>
  <done>PI-RL branch has explicit, validated scheduling controls that integrate with current learner loop counters and do not affect non-PI-RL runs.</done>
</task>

</tasks>

<verification>
- PI-RL recipe selection triggers learner PI-RL branch, not SAC branch.
- Replay/queue ingestion path remains the same for PI-RL and SAC runs.
- Existing non-PI-RL tests continue passing.
</verification>

<success_criteria>
- LRN-01 branch entry behavior is implemented and deterministic.
- LRN-02 reuse of existing replay/queue flow is preserved.
- No policy-type taxonomy changes are introduced.
</success_criteria>

<output>
After completion, create `.planning/phases/02-learner-pi-rl-training-path/02-01-SUMMARY.md`
</output>
